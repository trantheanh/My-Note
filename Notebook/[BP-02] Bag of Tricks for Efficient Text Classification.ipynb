{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[BP-02] Bag of Tricks for Efficient Text Classification.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOpf96LLDaLpizKp4KX4se5"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ADtSsz_9YzyV","colab_type":"text"},"source":["# 1. Introduction:"]},{"cell_type":"markdown","metadata":{"id":"EO5dsIEtMtFY","colab_type":"text"},"source":["- Text classification is an important task in NLP\n","- Model which achieve very good performance in practice tend to be relatively slow both train/inference time and limiting their use on very large dataset\n","- Linear classifier are often considered as strong baselines for text classfication problem. => obtain state-of-the-art performances if the right features are use \n","- Linear classifier can be scaled to very large-corpus\n","\n","==> Research is linear models with a rank constraint and a fast loss approximation (billion words can trained within ten minutes)"]},{"cell_type":"markdown","metadata":{"id":"r9yqpxHTMkhI","colab_type":"text"},"source":["# 2. Model architecture:"]},{"cell_type":"markdown","metadata":{"id":"WSeUguHGMwTQ","colab_type":"text"},"source":["- **Simple** and **efficient** baseline for sentence classification is to represent sentences as bag of words (**BOW**) and train a linear classifier (Logistic/Softmax, SVM)\n","- **Problem** with the classifier:\n","    - **Do not share** parameter (among features and classes) => limit generalization (when data is skewed)\n","- **Solution**: try to dense the representation of sentence which is sparse\n","    - factorize the linear classifier to low rank matrices \n","    - using multilayer neural network \n"]},{"cell_type":"markdown","metadata":{"id":"Vun46MNx5B_L","colab_type":"text"},"source":["![Fasttext Model](https://raw.githubusercontent.com/trantheanh/My-Note/master/Notebook/Image/Average_embedding.png)\n","\n","**One simple way for rank constraint:**\n","- Using embedding matrix to look up table over the words (or n-gram subwords)\n","- The word representation are then averaged into a text representation\n","=> feed to a linear classifier \n","\n","**Loss:**\n","- Minimize negative log-likelihood over the classes:\n","$$\n","- y log(f(BAx))\n","$$\n","- with B is weight matrix from hidden layer to output layer, and A is embedding matrix"]},{"cell_type":"markdown","metadata":{"id":"6PwvGQT4Rkuf","colab_type":"text"},"source":["# 3. Hierachical Softmax (HS):\n","- Complexity: $O(kh)$\n","  - $k$ is number of class\n","  - $h$ is dimension of the text representation\n","=> using HS make the complexity drop to $O(hlog_2(k)$ (base on binary tree)\n","- Prediction of class node at depth $l+1$ with parent $n_1, ..., n_l$:\n","$$ \n","P(n_{l+1}) = \\prod^l_{i=1} p(n_i)\n","$$"]},{"cell_type":"markdown","metadata":{"id":"AU1UI9NtB3F4","colab_type":"text"},"source":["# 4. N-gram features:\n","- Using subword level instead of word level"]},{"cell_type":"code","metadata":{"id":"zIwXFagjCGa3","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}